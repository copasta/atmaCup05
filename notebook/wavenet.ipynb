{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1591627808243,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "0wwTEOYC5gpJ",
    "outputId": "4046d7cf-b45c-4b17-f6eb-a63d0d9497d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  8 14:50:07 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taki8e9ZS3_W"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0uVoGo841zb"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtoHj-hvTIxD"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n",
    "from https://github.com/simon-larsson/keras-swa\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "class SWA(Callback):\n",
    "    \"\"\" Stochastic Weight Averging.\n",
    "    # Paper\n",
    "        title: Averaging Weights Leads to Wider Optima and Better Generalization\n",
    "        link: https://arxiv.org/abs/1803.05407\n",
    "    # Arguments\n",
    "        start_epoch:   integer, epoch when swa should start.\n",
    "        lr_schedule:   string, type of learning rate schedule.\n",
    "        swa_lr:        float, learning rate for swa sampling.\n",
    "        swa_lr2:       float, upper bound of cyclic learning rate.\n",
    "        swa_freq:      integer, length of learning rate cycle.\n",
    "        verbose:       integer, verbosity mode, 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 start_epoch,\n",
    "                 lr_schedule='manual',\n",
    "                 swa_lr='auto',\n",
    "                 swa_lr2='auto',\n",
    "                 swa_freq=1,\n",
    "                 verbose=0):\n",
    "                 \n",
    "        super(SWA, self).__init__()\n",
    "        self.start_epoch = start_epoch - 1\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_lr2 = swa_lr2\n",
    "        self.swa_freq = swa_freq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if start_epoch < 2:\n",
    "            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
    "\n",
    "        schedules = ['manual', 'constant', 'cyclic']\n",
    "\n",
    "        if self.lr_schedule not in schedules:\n",
    "            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n",
    "                             .format(self.lr_schedule))\n",
    "\n",
    "        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n",
    "            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
    "\n",
    "        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n",
    "            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n",
    "            \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n",
    "           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n",
    "            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.epochs = self.params.get('epochs')\n",
    "\n",
    "        if self.start_epoch >= self.epochs - 1:\n",
    "            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
    "\n",
    "        self.init_lr = K.eval(self.model.optimizer.lr)\n",
    "\n",
    "        # automatic swa_lr\n",
    "        if self.swa_lr == 'auto':\n",
    "            self.swa_lr = 0.1*self.init_lr\n",
    "        \n",
    "        if self.init_lr < self.swa_lr:\n",
    "            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
    "\n",
    "        # automatic swa_lr2 between initial lr and swa_lr   \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n",
    "            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n",
    "\n",
    "        self._check_batch_norm()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        self.current_epoch = epoch\n",
    "        self._scheduler(epoch)\n",
    "\n",
    "        # constant schedule is updated epoch-wise\n",
    "        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n",
    "            self._update_lr(epoch)\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: starting stochastic weight averaging'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            self._set_swa_weights(epoch)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: reinitializing batch normalization layers'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "            self._reset_batch_norm()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "\n",
    "        # update lr each batch for cyclic lr schedule\n",
    "        if self.lr_schedule == 'cyclic':\n",
    "            self._update_lr(self.current_epoch, batch)\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "\n",
    "            batch_size = self.params['samples']\n",
    "            momentum = batch_size / (batch*batch_size + batch_size)\n",
    "\n",
    "            for layer in self.batch_norm_layers:\n",
    "                layer.momentum = momentum\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.eval(self.model.optimizer.lr)\n",
    "        for k, v in logs.items():\n",
    "            if k == 'lr':\n",
    "                self.model.history.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_start_epoch = epoch\n",
    "\n",
    "        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n",
    "            self.swa_weights = self._average_weights(epoch)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "\n",
    "        if not self.has_batch_norm:\n",
    "            self._set_swa_weights(self.epochs)\n",
    "        else:\n",
    "            self._restore_batch_norm()\n",
    "\n",
    "    def _scheduler(self, epoch):\n",
    "\n",
    "        swa_epoch = (epoch - self.start_epoch)\n",
    "\n",
    "        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n",
    "        self.is_swa_start_epoch = epoch == self.start_epoch\n",
    "        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n",
    "\n",
    "    def _average_weights(self, epoch):\n",
    "\n",
    "        return [(swa_w * (epoch - self.start_epoch) + w)\n",
    "                / ((epoch - self.start_epoch) + 1)\n",
    "                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n",
    "\n",
    "    def _update_lr(self, epoch, batch=None):\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            lr = 0\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'constant':\n",
    "            lr = self._constant_schedule(epoch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'cyclic':\n",
    "            lr = self._cyclic_schedule(epoch, batch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def _constant_schedule(self, epoch):\n",
    "\n",
    "        t = epoch / self.start_epoch\n",
    "        lr_ratio = self.swa_lr / self.init_lr\n",
    "        if t <= 0.5:\n",
    "            factor = 1.0\n",
    "        elif t <= 0.9:\n",
    "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "        else:\n",
    "            factor = lr_ratio\n",
    "        return self.init_lr * factor\n",
    "\n",
    "    def _cyclic_schedule(self, epoch, batch):\n",
    "        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n",
    "        Wider Optima and Better Generalization(https://arxiv.org/abs/1803.05407)\n",
    "        \"\"\"\n",
    "        # steps are mini-batches per epoch, equal to training_samples / batch_size\n",
    "        steps = self.params.get('steps')\n",
    "        \n",
    "        #occasionally steps parameter will not be set. We then calculate it ourselves\n",
    "        if steps == None:\n",
    "            steps = self.params['samples'] // self.params['batch_size']\n",
    "        \n",
    "        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n",
    "        cycle_length = self.swa_freq * steps\n",
    "\n",
    "        # batch 0 indexed, so need to add 1\n",
    "        i = (swa_epoch * steps) + (batch + 1)\n",
    "        if epoch >= self.start_epoch:\n",
    "            t = (((i-1) % cycle_length) + 1)/cycle_length\n",
    "            return (1-t)*self.swa_lr2 + t*self.swa_lr\n",
    "        else:\n",
    "            return self._constant_schedule(epoch)\n",
    "\n",
    "    def _set_swa_weights(self, epoch):\n",
    "\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n",
    "                  % (epoch + 1))\n",
    "\n",
    "    def _check_batch_norm(self):\n",
    "\n",
    "        self.batch_norm_momentums = []\n",
    "        self.batch_norm_layers = []\n",
    "        self.has_batch_norm = False\n",
    "        self.running_bn_epoch = False\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            if issubclass(layer.__class__, BatchNormalization):\n",
    "                self.has_batch_norm = True\n",
    "                self.batch_norm_momentums.append(layer.momentum)\n",
    "                self.batch_norm_layers.append(layer)\n",
    "\n",
    "        if self.verbose > 0 and self.has_batch_norm:\n",
    "            print('Model uses batch normalization. SWA will require last epoch '\n",
    "                  'to be a forward pass and will run with no learning rate')\n",
    "\n",
    "    def _reset_batch_norm(self):\n",
    "\n",
    "        for layer in self.batch_norm_layers:\n",
    "\n",
    "            # to get properly initialized moving mean and moving variance weights\n",
    "            # we initialize a new batch norm layer from the config of the existing\n",
    "            # layer, build that layer, retrieve its reinitialized moving mean and\n",
    "            # moving var weights and then delete the layer\n",
    "            bn_config = layer.get_config()\n",
    "            new_batch_norm = BatchNormalization(**bn_config)\n",
    "            new_batch_norm.build(layer.input_shape)\n",
    "            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
    "            # get rid of the new_batch_norm layer\n",
    "            del new_batch_norm\n",
    "            # get the trained gamma and beta from the current batch norm layer\n",
    "            trained_weights = layer.get_weights()\n",
    "            new_weights = []\n",
    "            # get gamma if exists\n",
    "            if bn_config['scale']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            # get beta if exists\n",
    "            if bn_config['center']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            new_weights += [new_moving_mean, new_moving_var]\n",
    "            # set weights to trained gamma and beta, reinitialized mean and variance\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    def _restore_batch_norm(self):\n",
    "\n",
    "        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n",
    "            layer.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_U98VvSmVp6"
   },
   "outputs": [],
   "source": [
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWsEUOXvTYry"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/train_spec.csv')\n",
    "    test  = pd.read_csv('../input/test_spec.csv')\n",
    "    sub  = pd.read_csv('../input/atmaCup5__sample_submission.csv')\n",
    "    return train, test, sub\n",
    "\n",
    "def Scaling(train,test,features):\n",
    "    train_input_mean = train[features].mean(axis=1)\n",
    "    train_input_sigma = train[features].std(axis=1)\n",
    "    test_input_mean = test[features].mean(axis=1)\n",
    "    test_input_sigma = test[features].std(axis=1)\n",
    "    train[features]= train[features].sub(train_input_mean, axis=0).div(train_input_sigma, axis=0)\n",
    "    test[features] = test[features].sub(test_input_mean, axis=0).div(test_input_sigma, axis=0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ2z1yf3abO1"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1l7joOCeuiQX"
   },
   "outputs": [],
   "source": [
    "def Classifier():\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (512, 1))\n",
    "\n",
    "    x = wave_block(inp, 16, 5, 8)\n",
    "    x = wave_block(x, 32, 5, 12)\n",
    "    x = wave_block(x, 64, 5, 4)\n",
    "    x = wave_block(x, 128, 5, 1)\n",
    "\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_mean = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x_max, x_mean])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    out = Dense(1, activation = 'sigmoid', name = 'out')(x)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    opt = Adam(lr=LR)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[pr_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdURnaTqvPdj"
   },
   "outputs": [],
   "source": [
    "def run_cv_model_by_batch(train, test, splits, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "      \n",
    "    oof_ = np.zeros((len(train), len(target)))\n",
    "    preds_ = np.zeros((len(test), len(target)))\n",
    "    train['chip_id'], _ = pd.factorize(train['chip_id'])\n",
    "    splits = [x for x in stratified_group_k_fold(train, train.target, train.chip_id, k=SPLITS, seed=SEED)]\n",
    "    score_list = []\n",
    "\n",
    "    train_tr = train[target]\n",
    "    train = train[features]\n",
    "    test = test[features]\n",
    "    \n",
    "    for n_fold, (tr_idx, val_idx) in enumerate(splits):\n",
    "        train_x, train_y = train.iloc[tr_idx].values, train_tr.iloc[tr_idx].values\n",
    "        valid_x, valid_y = train.iloc[val_idx].values, train_tr.iloc[val_idx].values\n",
    "        train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], -1)\n",
    "        valid_x = valid_x.reshape(valid_x.shape[0], valid_x.shape[1], -1)\n",
    "        train_y = train_y.reshape(-1)\n",
    "        valid_y = valid_y.reshape(-1)\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        model = Classifier()\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(train_x, train_y,\n",
    "                  epochs = nn_epochs,\n",
    "                  callbacks = [swa, cb_lr_schedule],\n",
    "                  batch_size = nn_batch_size, verbose = 2,\n",
    "                  validation_data = (valid_x,valid_y))\n",
    "        #model.save_weights(f'./drive/My Drive/atmaCup05/output/ex03-WaveNet-{n_fold}.h5')\n",
    "        gc.collect()\n",
    "        preds_f = model.predict(valid_x)\n",
    "        score_ = average_precision_score(valid_y,  preds_f)\n",
    "        print(f'Training fold {n_fold + 1} completed. PR-AUC score : {score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_idx,:] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "\n",
    "        score_list.append(score_)\n",
    "    \n",
    "        \n",
    "    score_ = average_precision_score(train_tr,  oof_)\n",
    "    print(f'Training completed. oof PR-AUC score : {score_:1.5f}')\n",
    "    print(f'PR-AUC socre average : {np.mean(score_list):1.5f}, std : {np.std(score_list):1.5f}')\n",
    "    \n",
    "    np.save(f'../output/oof.{score_:1.5f}.npy',oof_)\n",
    "    np.save(f'../output/preds.{score_:1.5f}.npy',preds_)\n",
    "    \n",
    "    \n",
    "    sample_submission['target'] = preds_\n",
    "    sample_submission.to_csv(f'../output/submission_wavenet.{score_:1.5f}.csv', index=False)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    lr_precision, lr_recall, _ = metrics.precision_recall_curve(train_tr,  oof_)\n",
    "    ax.plot(lr_recall, lr_precision, marker='.', label='my prediction')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMiD5CeTTTR7"
   },
   "outputs": [],
   "source": [
    "## config\n",
    "EPOCHS = 15\n",
    "start_epoch = 10\n",
    "NNBATCHSIZE = 16\n",
    "SEED = 1234\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "target = ['target']\n",
    "\n",
    "swa = SWA(start_epoch=start_epoch, \n",
    "          lr_schedule='manual',\n",
    "          swa_lr=0.001, \n",
    "          verbose=1)\n",
    "\n",
    "pr_metric = AUC(curve='PR', num_thresholds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17747,
     "status": "ok",
     "timestamp": 1591627825108,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "U_uljJeMvedC",
    "outputId": "6fb3abba-5bd4-4e4b-92d5-2dfa46660815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading Data Started...')\n",
    "train, test, sample_submission = read_data()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ4Zpyfu5pUT"
   },
   "outputs": [],
   "source": [
    "features=[col for col in test.columns if \"wave\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfQRmbQ9v_DT"
   },
   "outputs": [],
   "source": [
    "train, test = Scaling(train, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3002094,
     "status": "ok",
     "timestamp": 1591630809499,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "Wrrccx2XwblD",
    "outputId": "c36a5846-46c0-4a56-ab1a-580d9d091cbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WaveNet model with 5 folds Started...\n",
      "Our training dataset shape is (5631, 512, 1)\n",
      "Our validation dataset shape is (1805, 512, 1)\n",
      "Epoch 1/15\n",
      "352/352 - 41s - loss: 0.1076 - auc: 0.2838 - lr: 0.0010 - val_loss: 0.0465 - val_auc: 0.9160\n",
      "Epoch 2/15\n",
      "352/352 - 36s - loss: 0.0624 - auc: 0.6496 - lr: 0.0010 - val_loss: 0.0352 - val_auc: 0.9214\n",
      "Epoch 3/15\n",
      "352/352 - 36s - loss: 0.0493 - auc: 0.7420 - lr: 0.0010 - val_loss: 0.0352 - val_auc: 0.9081\n",
      "Epoch 4/15\n",
      "352/352 - 36s - loss: 0.0486 - auc: 0.7156 - lr: 0.0010 - val_loss: 0.0383 - val_auc: 0.8920\n",
      "Epoch 5/15\n",
      "352/352 - 36s - loss: 0.0462 - auc: 0.7241 - lr: 0.0010 - val_loss: 0.0503 - val_auc: 0.9059\n",
      "Epoch 6/15\n",
      "352/352 - 36s - loss: 0.0430 - auc: 0.7531 - lr: 0.0010 - val_loss: 0.0384 - val_auc: 0.8980\n",
      "Epoch 7/15\n",
      "352/352 - 36s - loss: 0.0394 - auc: 0.8168 - lr: 0.0010 - val_loss: 0.0350 - val_auc: 0.9161\n",
      "Epoch 8/15\n",
      "352/352 - 36s - loss: 0.0343 - auc: 0.8164 - lr: 0.0010 - val_loss: 0.0480 - val_auc: 0.9094\n",
      "Epoch 9/15\n",
      "352/352 - 36s - loss: 0.0336 - auc: 0.8589 - lr: 0.0010 - val_loss: 0.0434 - val_auc: 0.9155\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "352/352 - 36s - loss: 0.0371 - auc: 0.8191 - lr: 0.0010 - val_loss: 0.0345 - val_auc: 0.9228\n",
      "Epoch 11/15\n",
      "352/352 - 36s - loss: 0.0301 - auc: 0.8505 - lr: 0.0010 - val_loss: 0.0379 - val_auc: 0.9129\n",
      "Epoch 12/15\n",
      "352/352 - 36s - loss: 0.0308 - auc: 0.8704 - lr: 0.0010 - val_loss: 0.0335 - val_auc: 0.9203\n",
      "Epoch 13/15\n",
      "352/352 - 36s - loss: 0.0290 - auc: 0.8721 - lr: 0.0010 - val_loss: 0.0331 - val_auc: 0.9100\n",
      "Epoch 14/15\n",
      "352/352 - 36s - loss: 0.0279 - auc: 0.8810 - lr: 0.0010 - val_loss: 0.0390 - val_auc: 0.9120\n",
      "Epoch 15/15\n",
      "352/352 - 36s - loss: 0.0325 - auc: 0.8589 - lr: 0.0010 - val_loss: 0.0342 - val_auc: 0.9090\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 1 completed. PR-AUC score : 0.92384\n",
      "Our training dataset shape is (5615, 512, 1)\n",
      "Our validation dataset shape is (1821, 512, 1)\n",
      "Epoch 1/15\n",
      "351/351 - 38s - loss: 0.0894 - auc: 0.4889 - lr: 0.0010 - val_loss: 0.2108 - val_auc: 0.6108\n",
      "Epoch 2/15\n",
      "351/351 - 36s - loss: 0.0620 - auc: 0.4832 - lr: 0.0010 - val_loss: 0.0869 - val_auc: 0.8728\n",
      "Epoch 3/15\n",
      "351/351 - 36s - loss: 0.0444 - auc: 0.6506 - lr: 0.0010 - val_loss: 0.0789 - val_auc: 0.8654\n",
      "Epoch 4/15\n",
      "351/351 - 36s - loss: 0.0359 - auc: 0.7600 - lr: 0.0010 - val_loss: 0.3620 - val_auc: 0.6172\n",
      "Epoch 5/15\n",
      "351/351 - 36s - loss: 0.0326 - auc: 0.7779 - lr: 0.0010 - val_loss: 0.0637 - val_auc: 0.8943\n",
      "Epoch 6/15\n",
      "351/351 - 36s - loss: 0.0304 - auc: 0.7968 - lr: 0.0010 - val_loss: 0.1043 - val_auc: 0.8500\n",
      "Epoch 7/15\n",
      "351/351 - 36s - loss: 0.0258 - auc: 0.8273 - lr: 0.0010 - val_loss: 0.1268 - val_auc: 0.8100\n",
      "Epoch 8/15\n",
      "351/351 - 36s - loss: 0.0329 - auc: 0.7739 - lr: 0.0010 - val_loss: 0.1359 - val_auc: 0.8596\n",
      "Epoch 9/15\n",
      "351/351 - 36s - loss: 0.0265 - auc: 0.8507 - lr: 0.0010 - val_loss: 0.0935 - val_auc: 0.8860\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "351/351 - 36s - loss: 0.0261 - auc: 0.8495 - lr: 0.0010 - val_loss: 0.1253 - val_auc: 0.8527\n",
      "Epoch 11/15\n",
      "351/351 - 36s - loss: 0.0261 - auc: 0.8353 - lr: 0.0010 - val_loss: 0.0682 - val_auc: 0.8848\n",
      "Epoch 12/15\n",
      "351/351 - 36s - loss: 0.0266 - auc: 0.8424 - lr: 0.0010 - val_loss: 0.0830 - val_auc: 0.8747\n",
      "Epoch 13/15\n",
      "351/351 - 36s - loss: 0.0248 - auc: 0.8381 - lr: 0.0010 - val_loss: 0.1664 - val_auc: 0.8026\n",
      "Epoch 14/15\n",
      "351/351 - 36s - loss: 0.0271 - auc: 0.8436 - lr: 0.0010 - val_loss: 0.1882 - val_auc: 0.8103\n",
      "Epoch 15/15\n",
      "351/351 - 36s - loss: 0.0218 - auc: 0.8728 - lr: 0.0010 - val_loss: 0.0860 - val_auc: 0.8637\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 2 completed. PR-AUC score : 0.88650\n",
      "Our training dataset shape is (6242, 512, 1)\n",
      "Our validation dataset shape is (1194, 512, 1)\n",
      "Epoch 1/15\n",
      "391/391 - 42s - loss: 0.1062 - auc: 0.6085 - lr: 0.0010 - val_loss: 0.0244 - val_auc: 0.9179\n",
      "Epoch 2/15\n",
      "391/391 - 39s - loss: 0.0637 - auc: 0.7369 - lr: 0.0010 - val_loss: 0.0249 - val_auc: 0.9193\n",
      "Epoch 3/15\n",
      "391/391 - 39s - loss: 0.0560 - auc: 0.7466 - lr: 0.0010 - val_loss: 0.0227 - val_auc: 0.9105\n",
      "Epoch 4/15\n",
      "391/391 - 39s - loss: 0.0518 - auc: 0.7871 - lr: 0.0010 - val_loss: 0.0157 - val_auc: 0.9464\n",
      "Epoch 5/15\n",
      "391/391 - 39s - loss: 0.0441 - auc: 0.8244 - lr: 0.0010 - val_loss: 0.0108 - val_auc: 0.9418\n",
      "Epoch 6/15\n",
      "391/391 - 39s - loss: 0.0414 - auc: 0.8384 - lr: 0.0010 - val_loss: 0.0127 - val_auc: 0.9258\n",
      "Epoch 7/15\n",
      "391/391 - 39s - loss: 0.0408 - auc: 0.8440 - lr: 0.0010 - val_loss: 0.0121 - val_auc: 0.9455\n",
      "Epoch 8/15\n",
      "391/391 - 39s - loss: 0.0447 - auc: 0.8292 - lr: 0.0010 - val_loss: 0.0137 - val_auc: 0.9624\n",
      "Epoch 9/15\n",
      "391/391 - 39s - loss: 0.0377 - auc: 0.8652 - lr: 0.0010 - val_loss: 0.0151 - val_auc: 0.9413\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "391/391 - 39s - loss: 0.0395 - auc: 0.8589 - lr: 0.0010 - val_loss: 0.0160 - val_auc: 0.8271\n",
      "Epoch 11/15\n",
      "391/391 - 39s - loss: 0.0367 - auc: 0.8786 - lr: 0.0010 - val_loss: 0.0111 - val_auc: 0.9261\n",
      "Epoch 12/15\n",
      "391/391 - 39s - loss: 0.0352 - auc: 0.8869 - lr: 0.0010 - val_loss: 0.0131 - val_auc: 0.8540\n",
      "Epoch 13/15\n",
      "391/391 - 39s - loss: 0.0372 - auc: 0.8733 - lr: 0.0010 - val_loss: 0.0090 - val_auc: 0.9327\n",
      "Epoch 14/15\n",
      "391/391 - 39s - loss: 0.0350 - auc: 0.8904 - lr: 0.0010 - val_loss: 0.0088 - val_auc: 0.9593\n",
      "Epoch 15/15\n",
      "391/391 - 39s - loss: 0.0327 - auc: 0.9018 - lr: 0.0010 - val_loss: 0.0120 - val_auc: 0.8922\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 3 completed. PR-AUC score : 0.92165\n",
      "Our training dataset shape is (6288, 512, 1)\n",
      "Our validation dataset shape is (1148, 512, 1)\n",
      "Epoch 1/15\n",
      "393/393 - 42s - loss: 0.0929 - auc: 0.5894 - lr: 0.0010 - val_loss: 0.0183 - val_auc: 0.7506\n",
      "Epoch 2/15\n",
      "393/393 - 39s - loss: 0.0562 - auc: 0.7620 - lr: 0.0010 - val_loss: 0.0298 - val_auc: 0.7313\n",
      "Epoch 3/15\n",
      "393/393 - 39s - loss: 0.0558 - auc: 0.7693 - lr: 0.0010 - val_loss: 0.0313 - val_auc: 0.5252\n",
      "Epoch 4/15\n",
      "393/393 - 39s - loss: 0.0526 - auc: 0.7756 - lr: 0.0010 - val_loss: 0.0213 - val_auc: 0.5671\n",
      "Epoch 5/15\n",
      "393/393 - 39s - loss: 0.0418 - auc: 0.8271 - lr: 0.0010 - val_loss: 0.0326 - val_auc: 0.5346\n",
      "Epoch 6/15\n",
      "393/393 - 39s - loss: 0.0476 - auc: 0.8044 - lr: 0.0010 - val_loss: 0.0244 - val_auc: 0.7153\n",
      "Epoch 7/15\n",
      "393/393 - 39s - loss: 0.0397 - auc: 0.8411 - lr: 0.0010 - val_loss: 0.0265 - val_auc: 0.5906\n",
      "Epoch 8/15\n",
      "393/393 - 39s - loss: 0.0407 - auc: 0.8383 - lr: 0.0010 - val_loss: 0.0259 - val_auc: 0.6038\n",
      "Epoch 9/15\n",
      "393/393 - 39s - loss: 0.0383 - auc: 0.8745 - lr: 0.0010 - val_loss: 0.0304 - val_auc: 0.5051\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "393/393 - 39s - loss: 0.0387 - auc: 0.8637 - lr: 0.0010 - val_loss: 0.0253 - val_auc: 0.6757\n",
      "Epoch 11/15\n",
      "393/393 - 40s - loss: 0.0361 - auc: 0.8676 - lr: 0.0010 - val_loss: 0.0219 - val_auc: 0.7124\n",
      "Epoch 12/15\n",
      "393/393 - 39s - loss: 0.0364 - auc: 0.8788 - lr: 0.0010 - val_loss: 0.0275 - val_auc: 0.7231\n",
      "Epoch 13/15\n",
      "393/393 - 39s - loss: 0.0336 - auc: 0.8881 - lr: 0.0010 - val_loss: 0.0265 - val_auc: 0.7629\n",
      "Epoch 14/15\n",
      "393/393 - 39s - loss: 0.0312 - auc: 0.8994 - lr: 0.0010 - val_loss: 0.0210 - val_auc: 0.6990\n",
      "Epoch 15/15\n",
      "393/393 - 39s - loss: 0.0380 - auc: 0.8826 - lr: 0.0010 - val_loss: 0.0268 - val_auc: 0.7115\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 4 completed. PR-AUC score : 0.76282\n",
      "Our training dataset shape is (5968, 512, 1)\n",
      "Our validation dataset shape is (1468, 512, 1)\n",
      "Epoch 1/15\n",
      "373/373 - 40s - loss: 0.1000 - auc: 0.5278 - lr: 0.0010 - val_loss: 0.0448 - val_auc: 0.6158\n",
      "Epoch 2/15\n",
      "373/373 - 38s - loss: 0.0620 - auc: 0.7403 - lr: 0.0010 - val_loss: 0.0419 - val_auc: 0.5288\n",
      "Epoch 3/15\n",
      "373/373 - 38s - loss: 0.0545 - auc: 0.7777 - lr: 0.0010 - val_loss: 0.0481 - val_auc: 0.6409\n",
      "Epoch 4/15\n",
      "373/373 - 38s - loss: 0.0518 - auc: 0.7996 - lr: 0.0010 - val_loss: 0.0339 - val_auc: 0.5405\n",
      "Epoch 5/15\n",
      "373/373 - 38s - loss: 0.0492 - auc: 0.7866 - lr: 0.0010 - val_loss: 0.0404 - val_auc: 0.6071\n",
      "Epoch 6/15\n",
      "373/373 - 38s - loss: 0.0436 - auc: 0.8522 - lr: 0.0010 - val_loss: 0.0297 - val_auc: 0.6330\n",
      "Epoch 7/15\n",
      "373/373 - 38s - loss: 0.0403 - auc: 0.8402 - lr: 0.0010 - val_loss: 0.0551 - val_auc: 0.4826\n",
      "Epoch 8/15\n",
      "373/373 - 38s - loss: 0.0427 - auc: 0.8476 - lr: 0.0010 - val_loss: 0.0355 - val_auc: 0.5781\n",
      "Epoch 9/15\n",
      "373/373 - 38s - loss: 0.0468 - auc: 0.8278 - lr: 0.0010 - val_loss: 0.0311 - val_auc: 0.7076\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "373/373 - 38s - loss: 0.0342 - auc: 0.8839 - lr: 0.0010 - val_loss: 0.0443 - val_auc: 0.7423\n",
      "Epoch 11/15\n",
      "373/373 - 38s - loss: 0.0338 - auc: 0.8854 - lr: 0.0010 - val_loss: 0.0284 - val_auc: 0.7218\n",
      "Epoch 12/15\n",
      "373/373 - 38s - loss: 0.0355 - auc: 0.8933 - lr: 0.0010 - val_loss: 0.0370 - val_auc: 0.7016\n",
      "Epoch 13/15\n",
      "373/373 - 38s - loss: 0.0371 - auc: 0.8671 - lr: 0.0010 - val_loss: 0.0291 - val_auc: 0.7784\n",
      "Epoch 14/15\n",
      "373/373 - 38s - loss: 0.0346 - auc: 0.8937 - lr: 0.0010 - val_loss: 0.0266 - val_auc: 0.7155\n",
      "Epoch 15/15\n",
      "373/373 - 38s - loss: 0.0374 - auc: 0.8557 - lr: 0.0010 - val_loss: 0.0268 - val_auc: 0.6894\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 5 completed. PR-AUC score : 0.81491\n",
      "Training completed. oof PR-AUC score : 0.86263\n",
      "PR-AUC socre average : 0.86194, std : 0.06330\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "print(f'Training WaveNet model with {SPLITS} folds Started...')\n",
    "run_cv_model_by_batch(train, test, SPLITS, features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "print('Training completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFojy20r781y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOxGkBrGJ8/Po02AJ1Dl7TS",
   "collapsed_sections": [],
   "mount_file_id": "1vsd2ryy0kci_aYNgbRLQu0kvoxviksHb",
   "name": "wavenet_ex03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
