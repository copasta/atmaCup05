{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1591630986992,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "-pgnOVI7LepI",
    "outputId": "af3133f2-a072-40ae-cee2-08fc6b990638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  8 15:43:06 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taki8e9ZS3_W"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fpcgGmsLsbg"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtoHj-hvTIxD"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n",
    "from https://github.com/simon-larsson/keras-swa\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "class SWA(Callback):\n",
    "    \"\"\" Stochastic Weight Averging.\n",
    "    # Paper\n",
    "        title: Averaging Weights Leads to Wider Optima and Better Generalization\n",
    "        link: https://arxiv.org/abs/1803.05407\n",
    "    # Arguments\n",
    "        start_epoch:   integer, epoch when swa should start.\n",
    "        lr_schedule:   string, type of learning rate schedule.\n",
    "        swa_lr:        float, learning rate for swa sampling.\n",
    "        swa_lr2:       float, upper bound of cyclic learning rate.\n",
    "        swa_freq:      integer, length of learning rate cycle.\n",
    "        verbose:       integer, verbosity mode, 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 start_epoch,\n",
    "                 lr_schedule='manual',\n",
    "                 swa_lr='auto',\n",
    "                 swa_lr2='auto',\n",
    "                 swa_freq=1,\n",
    "                 verbose=0):\n",
    "                 \n",
    "        super(SWA, self).__init__()\n",
    "        self.start_epoch = start_epoch - 1\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_lr2 = swa_lr2\n",
    "        self.swa_freq = swa_freq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if start_epoch < 2:\n",
    "            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
    "\n",
    "        schedules = ['manual', 'constant', 'cyclic']\n",
    "\n",
    "        if self.lr_schedule not in schedules:\n",
    "            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n",
    "                             .format(self.lr_schedule))\n",
    "\n",
    "        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n",
    "            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
    "\n",
    "        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n",
    "            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n",
    "            \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n",
    "           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n",
    "            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.epochs = self.params.get('epochs')\n",
    "\n",
    "        if self.start_epoch >= self.epochs - 1:\n",
    "            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
    "\n",
    "        self.init_lr = K.eval(self.model.optimizer.lr)\n",
    "\n",
    "        # automatic swa_lr\n",
    "        if self.swa_lr == 'auto':\n",
    "            self.swa_lr = 0.1*self.init_lr\n",
    "        \n",
    "        if self.init_lr < self.swa_lr:\n",
    "            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
    "\n",
    "        # automatic swa_lr2 between initial lr and swa_lr   \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n",
    "            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n",
    "\n",
    "        self._check_batch_norm()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        self.current_epoch = epoch\n",
    "        self._scheduler(epoch)\n",
    "\n",
    "        # constant schedule is updated epoch-wise\n",
    "        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n",
    "            self._update_lr(epoch)\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: starting stochastic weight averaging'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            self._set_swa_weights(epoch)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: reinitializing batch normalization layers'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "            self._reset_batch_norm()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "\n",
    "        # update lr each batch for cyclic lr schedule\n",
    "        if self.lr_schedule == 'cyclic':\n",
    "            self._update_lr(self.current_epoch, batch)\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "\n",
    "            batch_size = self.params['samples']\n",
    "            momentum = batch_size / (batch*batch_size + batch_size)\n",
    "\n",
    "            for layer in self.batch_norm_layers:\n",
    "                layer.momentum = momentum\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.eval(self.model.optimizer.lr)\n",
    "        for k, v in logs.items():\n",
    "            if k == 'lr':\n",
    "                self.model.history.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_start_epoch = epoch\n",
    "\n",
    "        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n",
    "            self.swa_weights = self._average_weights(epoch)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "\n",
    "        if not self.has_batch_norm:\n",
    "            self._set_swa_weights(self.epochs)\n",
    "        else:\n",
    "            self._restore_batch_norm()\n",
    "\n",
    "    def _scheduler(self, epoch):\n",
    "\n",
    "        swa_epoch = (epoch - self.start_epoch)\n",
    "\n",
    "        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n",
    "        self.is_swa_start_epoch = epoch == self.start_epoch\n",
    "        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n",
    "\n",
    "    def _average_weights(self, epoch):\n",
    "\n",
    "        return [(swa_w * (epoch - self.start_epoch) + w)\n",
    "                / ((epoch - self.start_epoch) + 1)\n",
    "                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n",
    "\n",
    "    def _update_lr(self, epoch, batch=None):\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            lr = 0\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'constant':\n",
    "            lr = self._constant_schedule(epoch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'cyclic':\n",
    "            lr = self._cyclic_schedule(epoch, batch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def _constant_schedule(self, epoch):\n",
    "\n",
    "        t = epoch / self.start_epoch\n",
    "        lr_ratio = self.swa_lr / self.init_lr\n",
    "        if t <= 0.5:\n",
    "            factor = 1.0\n",
    "        elif t <= 0.9:\n",
    "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "        else:\n",
    "            factor = lr_ratio\n",
    "        return self.init_lr * factor\n",
    "\n",
    "    def _cyclic_schedule(self, epoch, batch):\n",
    "        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n",
    "        Wider Optima and Better Generalization(https://arxiv.org/abs/1803.05407)\n",
    "        \"\"\"\n",
    "        # steps are mini-batches per epoch, equal to training_samples / batch_size\n",
    "        steps = self.params.get('steps')\n",
    "        \n",
    "        #occasionally steps parameter will not be set. We then calculate it ourselves\n",
    "        if steps == None:\n",
    "            steps = self.params['samples'] // self.params['batch_size']\n",
    "        \n",
    "        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n",
    "        cycle_length = self.swa_freq * steps\n",
    "\n",
    "        # batch 0 indexed, so need to add 1\n",
    "        i = (swa_epoch * steps) + (batch + 1)\n",
    "        if epoch >= self.start_epoch:\n",
    "            t = (((i-1) % cycle_length) + 1)/cycle_length\n",
    "            return (1-t)*self.swa_lr2 + t*self.swa_lr\n",
    "        else:\n",
    "            return self._constant_schedule(epoch)\n",
    "\n",
    "    def _set_swa_weights(self, epoch):\n",
    "\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n",
    "                  % (epoch + 1))\n",
    "\n",
    "    def _check_batch_norm(self):\n",
    "\n",
    "        self.batch_norm_momentums = []\n",
    "        self.batch_norm_layers = []\n",
    "        self.has_batch_norm = False\n",
    "        self.running_bn_epoch = False\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            if issubclass(layer.__class__, BatchNormalization):\n",
    "                self.has_batch_norm = True\n",
    "                self.batch_norm_momentums.append(layer.momentum)\n",
    "                self.batch_norm_layers.append(layer)\n",
    "\n",
    "        if self.verbose > 0 and self.has_batch_norm:\n",
    "            print('Model uses batch normalization. SWA will require last epoch '\n",
    "                  'to be a forward pass and will run with no learning rate')\n",
    "\n",
    "    def _reset_batch_norm(self):\n",
    "\n",
    "        for layer in self.batch_norm_layers:\n",
    "\n",
    "            # to get properly initialized moving mean and moving variance weights\n",
    "            # we initialize a new batch norm layer from the config of the existing\n",
    "            # layer, build that layer, retrieve its reinitialized moving mean and\n",
    "            # moving var weights and then delete the layer\n",
    "            bn_config = layer.get_config()\n",
    "            new_batch_norm = BatchNormalization(**bn_config)\n",
    "            new_batch_norm.build(layer.input_shape)\n",
    "            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
    "            # get rid of the new_batch_norm layer\n",
    "            del new_batch_norm\n",
    "            # get the trained gamma and beta from the current batch norm layer\n",
    "            trained_weights = layer.get_weights()\n",
    "            new_weights = []\n",
    "            # get gamma if exists\n",
    "            if bn_config['scale']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            # get beta if exists\n",
    "            if bn_config['center']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            new_weights += [new_moving_mean, new_moving_var]\n",
    "            # set weights to trained gamma and beta, reinitialized mean and variance\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    def _restore_batch_norm(self):\n",
    "\n",
    "        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n",
    "            layer.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_U98VvSmVp6"
   },
   "outputs": [],
   "source": [
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWsEUOXvTYry"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/train_spec.csv')\n",
    "    test  = pd.read_csv('../input/test_spec.csv')\n",
    "    sub  = pd.read_csv('../input/atmaCup5__sample_submission.csv')\n",
    "    return train, test, sub\n",
    "\n",
    "def Scaling(train,test,features):\n",
    "    train_input_mean = train[features].mean(axis=1)\n",
    "    train_input_sigma = train[features].std(axis=1)\n",
    "    test_input_mean = test[features].mean(axis=1)\n",
    "    test_input_sigma = test[features].std(axis=1)\n",
    "    train[features]= train[features].sub(train_input_mean, axis=0).div(train_input_sigma, axis=0)\n",
    "    test[features] = test[features].sub(test_input_mean, axis=0).div(test_input_sigma, axis=0)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ2z1yf3abO1"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1l7joOCeuiQX"
   },
   "outputs": [],
   "source": [
    "def Classifier():\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (512, 1))\n",
    "\n",
    "    x = wave_block(inp, 16, 5, 8)\n",
    "    x = wave_block(x, 32, 5, 12)\n",
    "    x = wave_block(x, 64, 5, 4)\n",
    "    x = wave_block(x, 128, 5, 1)\n",
    "\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_mean = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x_max, x_mean])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    out = Dense(1, activation = 'sigmoid', name = 'out')(x)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    opt = Adam(lr=LR)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[pr_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdURnaTqvPdj"
   },
   "outputs": [],
   "source": [
    "def run_cv_model_by_batch(train, test, splits, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "      \n",
    "    oof_ = np.zeros((len(train), len(target)))\n",
    "    preds_ = np.zeros((len(test), len(target)))\n",
    "    train['chip_id'], _ = pd.factorize(train['chip_id'])\n",
    "    splits = [x for x in stratified_group_k_fold(train, train.target.astype(int), train.chip_id, k=SPLITS, seed=SEED)]\n",
    "    score_list = []\n",
    "\n",
    "    train_tr = train[target]\n",
    "    train = train[features]\n",
    "    test = test[features]\n",
    "    \n",
    "    for n_fold, (tr_idx, val_idx) in enumerate(splits):\n",
    "        train_x, train_y = train.iloc[tr_idx].values, train_tr.iloc[tr_idx].values\n",
    "        valid_x, valid_y = train.iloc[val_idx].values, train_tr.iloc[val_idx].values\n",
    "        train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], -1)\n",
    "        valid_x = valid_x.reshape(valid_x.shape[0], valid_x.shape[1], -1)\n",
    "        train_y = train_y.reshape(-1)\n",
    "        valid_y = valid_y.reshape(-1)\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        model = Classifier()\n",
    "\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(train_x, train_y,\n",
    "                  epochs = nn_epochs,\n",
    "                  callbacks = [swa, cb_lr_schedule],\n",
    "                  batch_size = nn_batch_size, verbose = 2,\n",
    "                  validation_data = (valid_x,valid_y))\n",
    "        #model.save_weights(f'../output/ex10-WaveNet-{n_fold}.h5')\n",
    "        gc.collect()\n",
    "        preds_f = model.predict(valid_x)\n",
    "        score_ = average_precision_score(valid_y,  preds_f)\n",
    "        print(f'Training fold {n_fold + 1} completed. PR-AUC score : {score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_idx,:] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "\n",
    "        score_list.append(score_)\n",
    "\n",
    "    score_ = average_precision_score(train_tr,  oof_)\n",
    "    print(f'Training completed. oof PR-AUC score : {score_:1.5f}')\n",
    "    print(f'PR-AUC socre average : {np.mean(score_list):1.5f}, std : {np.std(score_list):1.5f}')\n",
    "    \n",
    "    np.save(f'../output/oof.{score_:1.5f}_pseudo_labeling.npy',oof_)\n",
    "    np.save(f'../output/preds.{score_:1.5f}_pseudo_labeling.npy',preds_)\n",
    "    \n",
    "    \n",
    "    sample_submission['target'] = preds_\n",
    "    sample_submission.to_csv(f'../output/submission_wavenet.{score_:1.5f}_pseudo_labeling.csv', index=False)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    lr_precision, lr_recall, _ = metrics.precision_recall_curve(train_tr,  oof_)\n",
    "    ax.plot(lr_recall, lr_precision, marker='.', label='my prediction')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMiD5CeTTTR7"
   },
   "outputs": [],
   "source": [
    "## config\n",
    "EPOCHS = 15\n",
    "start_epoch = 10\n",
    "NNBATCHSIZE = 16\n",
    "SEED = 1234\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "target = ['target']\n",
    "\n",
    "swa = SWA(start_epoch=start_epoch, \n",
    "          lr_schedule='manual',\n",
    "          swa_lr=0.001, \n",
    "          verbose=1)\n",
    "\n",
    "pr_metric = AUC(curve='PR', num_thresholds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4678,
     "status": "ok",
     "timestamp": 1591631000565,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "U_uljJeMvedC",
    "outputId": "ff189ba2-372e-4a65-e9ad-b2a82019fbdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading Data Started...')\n",
    "train, test, sample_submission = read_data()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjhW0Gq8Y6YV"
   },
   "outputs": [],
   "source": [
    "oof = np.load(\"../output/oof.0.86263.npy\")\n",
    "sub = np.load(\"../output/preds.0.86263.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1591631013877,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "wzYFGdZ0ZrjR",
    "outputId": "61cab6f9-20d6-4701-ab02-32b50d6248cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([9]), array([6280]), array([6289]), 6952)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((sub > 0.99)), sum((sub < 0.01)), sum((sub > 0.99) | (sub < 0.01)), len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1591631019072,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "pTbNtivGbOpk",
    "outputId": "bd6d5d05-8ce0-4152-a269-89f6a08729c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6289, 521) (6952, 520)\n"
     ]
    }
   ],
   "source": [
    "pused_test = test.copy()\n",
    "pused_test['target'] = sub\n",
    "pused_test = pused_test[(sub >= 0.99) | (sub <= 0.01)].copy().reset_index(drop=True)\n",
    "pused_test.loc[ pused_test['target']>=0.5, 'target' ] = 1\n",
    "pused_test.loc[ pused_test['target']<0.5, 'target' ] = 0 \n",
    "print(pused_test.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8748,
     "status": "ok",
     "timestamp": 1591631034244,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "5MYYg5zvcItr",
    "outputId": "5c50fdea-e88a-4130-9519-51847cab3a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7436, 521)\n",
      "(13725, 521)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train = pd.concat([train, pused_test], axis=0).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "print(train.shape)\n",
    "#train.to_csv(\"../output/train_pseudo_labeling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ4Zpyfu5pUT"
   },
   "outputs": [],
   "source": [
    "features=[col for col in test.columns if \"wave\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfQRmbQ9v_DT"
   },
   "outputs": [],
   "source": [
    "train, test = Scaling(train, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5354137,
     "status": "ok",
     "timestamp": 1591636395979,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "Wrrccx2XwblD",
    "outputId": "a7340efe-abf0-4891-e97a-f717cfa729b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Conv1d model with 5 folds Started...\n",
      "Our training dataset shape is (11460, 512, 1)\n",
      "Our validation dataset shape is (2265, 512, 1)\n",
      "Epoch 1/15\n",
      "717/717 - 75s - loss: 0.0599 - auc: 0.3320 - lr: 0.0010 - val_loss: 0.0345 - val_auc: 0.8924\n",
      "Epoch 2/15\n",
      "717/717 - 71s - loss: 0.0384 - auc: 0.6509 - lr: 0.0010 - val_loss: 0.0522 - val_auc: 0.9137\n",
      "Epoch 3/15\n",
      "717/717 - 71s - loss: 0.0292 - auc: 0.6782 - lr: 0.0010 - val_loss: 0.0310 - val_auc: 0.9072\n",
      "Epoch 4/15\n",
      "717/717 - 71s - loss: 0.0249 - auc: 0.7563 - lr: 0.0010 - val_loss: 0.0314 - val_auc: 0.8890\n",
      "Epoch 5/15\n",
      "717/717 - 71s - loss: 0.0231 - auc: 0.7953 - lr: 0.0010 - val_loss: 0.0339 - val_auc: 0.8977\n",
      "Epoch 6/15\n",
      "717/717 - 71s - loss: 0.0222 - auc: 0.7809 - lr: 0.0010 - val_loss: 0.0278 - val_auc: 0.9158\n",
      "Epoch 7/15\n",
      "717/717 - 72s - loss: 0.0215 - auc: 0.8160 - lr: 0.0010 - val_loss: 0.0282 - val_auc: 0.9146\n",
      "Epoch 8/15\n",
      "717/717 - 71s - loss: 0.0232 - auc: 0.7940 - lr: 0.0010 - val_loss: 0.0297 - val_auc: 0.9121\n",
      "Epoch 9/15\n",
      "717/717 - 71s - loss: 0.0185 - auc: 0.8228 - lr: 0.0010 - val_loss: 0.0244 - val_auc: 0.9252\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "717/717 - 71s - loss: 0.0194 - auc: 0.8079 - lr: 0.0010 - val_loss: 0.0336 - val_auc: 0.9020\n",
      "Epoch 11/15\n",
      "717/717 - 71s - loss: 0.0178 - auc: 0.8340 - lr: 0.0010 - val_loss: 0.0316 - val_auc: 0.9170\n",
      "Epoch 12/15\n",
      "717/717 - 71s - loss: 0.0179 - auc: 0.8573 - lr: 0.0010 - val_loss: 0.0393 - val_auc: 0.8813\n",
      "Epoch 13/15\n",
      "717/717 - 71s - loss: 0.0253 - auc: 0.7744 - lr: 0.0010 - val_loss: 0.0484 - val_auc: 0.9041\n",
      "Epoch 14/15\n",
      "717/717 - 71s - loss: 0.0217 - auc: 0.8100 - lr: 0.0010 - val_loss: 0.0250 - val_auc: 0.9224\n",
      "Epoch 15/15\n",
      "717/717 - 71s - loss: 0.0183 - auc: 0.8498 - lr: 0.0010 - val_loss: 0.0297 - val_auc: 0.9185\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 1 completed. PR-AUC score : 0.91615\n",
      "Our training dataset shape is (11706, 512, 1)\n",
      "Our validation dataset shape is (2019, 512, 1)\n",
      "Epoch 1/15\n",
      "732/732 - 76s - loss: 0.0528 - auc: 0.4658 - lr: 0.0010 - val_loss: 0.3508 - val_auc: 0.6888\n",
      "Epoch 2/15\n",
      "732/732 - 72s - loss: 0.0296 - auc: 0.5723 - lr: 0.0010 - val_loss: 0.1729 - val_auc: 0.8233\n",
      "Epoch 3/15\n",
      "732/732 - 72s - loss: 0.0239 - auc: 0.7033 - lr: 0.0010 - val_loss: 0.0923 - val_auc: 0.8452\n",
      "Epoch 4/15\n",
      "732/732 - 72s - loss: 0.0186 - auc: 0.7584 - lr: 0.0010 - val_loss: 0.0977 - val_auc: 0.8583\n",
      "Epoch 5/15\n",
      "732/732 - 72s - loss: 0.0165 - auc: 0.8291 - lr: 0.0010 - val_loss: 0.0764 - val_auc: 0.8485\n",
      "Epoch 6/15\n",
      "732/732 - 72s - loss: 0.0173 - auc: 0.7621 - lr: 0.0010 - val_loss: 0.0875 - val_auc: 0.8419\n",
      "Epoch 7/15\n",
      "732/732 - 72s - loss: 0.0166 - auc: 0.7846 - lr: 0.0010 - val_loss: 0.0595 - val_auc: 0.8925\n",
      "Epoch 8/15\n",
      "732/732 - 72s - loss: 0.0180 - auc: 0.8032 - lr: 0.0010 - val_loss: 0.0825 - val_auc: 0.8819\n",
      "Epoch 9/15\n",
      "732/732 - 72s - loss: 0.0157 - auc: 0.7991 - lr: 0.0010 - val_loss: 0.0840 - val_auc: 0.8677\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "732/732 - 72s - loss: 0.0149 - auc: 0.8282 - lr: 0.0010 - val_loss: 0.0672 - val_auc: 0.8578\n",
      "Epoch 11/15\n",
      "732/732 - 72s - loss: 0.0174 - auc: 0.7876 - lr: 0.0010 - val_loss: 0.1030 - val_auc: 0.8573\n",
      "Epoch 12/15\n",
      "732/732 - 72s - loss: 0.0146 - auc: 0.8259 - lr: 0.0010 - val_loss: 0.1376 - val_auc: 0.8098\n",
      "Epoch 13/15\n",
      "732/732 - 72s - loss: 0.0143 - auc: 0.8390 - lr: 0.0010 - val_loss: 0.0614 - val_auc: 0.8732\n",
      "Epoch 14/15\n",
      "732/732 - 72s - loss: 0.0124 - auc: 0.8863 - lr: 0.0010 - val_loss: 0.1237 - val_auc: 0.8306\n",
      "Epoch 15/15\n",
      "732/732 - 72s - loss: 0.0135 - auc: 0.8642 - lr: 0.0010 - val_loss: 0.1194 - val_auc: 0.7848\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 2 completed. PR-AUC score : 0.87557\n",
      "Our training dataset shape is (10376, 512, 1)\n",
      "Our validation dataset shape is (3349, 512, 1)\n",
      "Epoch 1/15\n",
      "649/649 - 70s - loss: 0.0793 - auc: 0.5057 - lr: 0.0010 - val_loss: 0.0014 - val_auc: 1.0000\n",
      "Epoch 2/15\n",
      "649/649 - 67s - loss: 0.0456 - auc: 0.6694 - lr: 0.0010 - val_loss: 0.0036 - val_auc: 1.0000\n",
      "Epoch 3/15\n",
      "649/649 - 67s - loss: 0.0350 - auc: 0.7630 - lr: 0.0010 - val_loss: 0.0019 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "649/649 - 67s - loss: 0.0362 - auc: 0.7709 - lr: 0.0010 - val_loss: 1.0245e-04 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "649/649 - 67s - loss: 0.0296 - auc: 0.8092 - lr: 0.0010 - val_loss: 3.3127e-04 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "649/649 - 67s - loss: 0.0312 - auc: 0.8004 - lr: 0.0010 - val_loss: 1.0385e-04 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "649/649 - 67s - loss: 0.0274 - auc: 0.8367 - lr: 0.0010 - val_loss: 2.0250e-04 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "649/649 - 67s - loss: 0.0294 - auc: 0.8214 - lr: 0.0010 - val_loss: 4.3866e-04 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "649/649 - 67s - loss: 0.0264 - auc: 0.8573 - lr: 0.0010 - val_loss: 0.0010 - val_auc: 1.0000\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "649/649 - 67s - loss: 0.0247 - auc: 0.8602 - lr: 0.0010 - val_loss: 2.1076e-05 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "649/649 - 67s - loss: 0.0272 - auc: 0.8393 - lr: 0.0010 - val_loss: 3.5614e-04 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "649/649 - 67s - loss: 0.0250 - auc: 0.8624 - lr: 0.0010 - val_loss: 2.2395e-04 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "649/649 - 67s - loss: 0.0225 - auc: 0.8740 - lr: 0.0010 - val_loss: 0.0053 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "649/649 - 67s - loss: 0.0258 - auc: 0.8631 - lr: 0.0010 - val_loss: 0.0026 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "649/649 - 66s - loss: 0.0205 - auc: 0.8997 - lr: 0.0010 - val_loss: 4.0400e-05 - val_auc: 1.0000\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 3 completed. PR-AUC score : 1.00000\n",
      "Our training dataset shape is (11383, 512, 1)\n",
      "Our validation dataset shape is (2342, 512, 1)\n",
      "Epoch 1/15\n",
      "712/712 - 74s - loss: 0.0572 - auc: 0.5558 - lr: 0.0010 - val_loss: 0.0326 - val_auc: 0.7723\n",
      "Epoch 2/15\n",
      "712/712 - 70s - loss: 0.0371 - auc: 0.7317 - lr: 0.0010 - val_loss: 0.0421 - val_auc: 0.8381\n",
      "Epoch 3/15\n",
      "712/712 - 71s - loss: 0.0320 - auc: 0.7624 - lr: 0.0010 - val_loss: 0.0162 - val_auc: 0.8347\n",
      "Epoch 4/15\n",
      "712/712 - 71s - loss: 0.0315 - auc: 0.7610 - lr: 0.0010 - val_loss: 0.0326 - val_auc: 0.7322\n",
      "Epoch 5/15\n",
      "712/712 - 71s - loss: 0.0273 - auc: 0.8108 - lr: 0.0010 - val_loss: 0.0184 - val_auc: 0.8551\n",
      "Epoch 6/15\n",
      "712/712 - 71s - loss: 0.0275 - auc: 0.8075 - lr: 0.0010 - val_loss: 0.0196 - val_auc: 0.7677\n",
      "Epoch 7/15\n",
      "712/712 - 71s - loss: 0.0245 - auc: 0.8290 - lr: 0.0010 - val_loss: 0.0151 - val_auc: 0.7750\n",
      "Epoch 8/15\n",
      "712/712 - 71s - loss: 0.0218 - auc: 0.8595 - lr: 0.0010 - val_loss: 0.0205 - val_auc: 0.7792\n",
      "Epoch 9/15\n",
      "712/712 - 71s - loss: 0.0221 - auc: 0.8445 - lr: 0.0010 - val_loss: 0.0166 - val_auc: 0.8133\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "712/712 - 71s - loss: 0.0241 - auc: 0.8308 - lr: 0.0010 - val_loss: 0.0500 - val_auc: 0.7335\n",
      "Epoch 11/15\n",
      "712/712 - 71s - loss: 0.0194 - auc: 0.8799 - lr: 0.0010 - val_loss: 0.0254 - val_auc: 0.6960\n",
      "Epoch 12/15\n",
      "712/712 - 71s - loss: 0.0211 - auc: 0.8640 - lr: 0.0010 - val_loss: 0.0216 - val_auc: 0.7659\n",
      "Epoch 13/15\n",
      "712/712 - 71s - loss: 0.0202 - auc: 0.8624 - lr: 0.0010 - val_loss: 0.0214 - val_auc: 0.7211\n",
      "Epoch 14/15\n",
      "712/712 - 71s - loss: 0.0193 - auc: 0.8688 - lr: 0.0010 - val_loss: 0.0181 - val_auc: 0.7803\n",
      "Epoch 15/15\n",
      "712/712 - 71s - loss: 0.0195 - auc: 0.8810 - lr: 0.0010 - val_loss: 0.0244 - val_auc: 0.7400\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 4 completed. PR-AUC score : 0.82591\n",
      "Our training dataset shape is (9975, 512, 1)\n",
      "Our validation dataset shape is (3750, 512, 1)\n",
      "Epoch 1/15\n",
      "624/624 - 67s - loss: 0.0618 - auc: 0.5781 - lr: 0.0010 - val_loss: 0.0262 - val_auc: 0.5713\n",
      "Epoch 2/15\n",
      "624/624 - 65s - loss: 0.0369 - auc: 0.7718 - lr: 0.0010 - val_loss: 0.0178 - val_auc: 0.4031\n",
      "Epoch 3/15\n",
      "624/624 - 65s - loss: 0.0361 - auc: 0.7871 - lr: 0.0010 - val_loss: 0.0124 - val_auc: 0.6481\n",
      "Epoch 4/15\n",
      "624/624 - 65s - loss: 0.0315 - auc: 0.8226 - lr: 0.0010 - val_loss: 0.0124 - val_auc: 0.6961\n",
      "Epoch 5/15\n",
      "624/624 - 65s - loss: 0.0283 - auc: 0.8444 - lr: 0.0010 - val_loss: 0.0100 - val_auc: 0.8142\n",
      "Epoch 6/15\n",
      "624/624 - 65s - loss: 0.0267 - auc: 0.8695 - lr: 0.0010 - val_loss: 0.0184 - val_auc: 0.6302\n",
      "Epoch 7/15\n",
      "624/624 - 65s - loss: 0.0296 - auc: 0.8334 - lr: 0.0010 - val_loss: 0.0104 - val_auc: 0.7379\n",
      "Epoch 8/15\n",
      "624/624 - 65s - loss: 0.0279 - auc: 0.8488 - lr: 0.0010 - val_loss: 0.0335 - val_auc: 0.5980\n",
      "Epoch 9/15\n",
      "624/624 - 65s - loss: 0.0261 - auc: 0.8542 - lr: 0.0010 - val_loss: 0.0086 - val_auc: 0.8034\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "624/624 - 65s - loss: 0.0267 - auc: 0.8502 - lr: 0.0010 - val_loss: 0.0107 - val_auc: 0.6768\n",
      "Epoch 11/15\n",
      "624/624 - 65s - loss: 0.0241 - auc: 0.8709 - lr: 0.0010 - val_loss: 0.0081 - val_auc: 0.7946\n",
      "Epoch 12/15\n",
      "624/624 - 65s - loss: 0.0257 - auc: 0.8595 - lr: 0.0010 - val_loss: 0.0089 - val_auc: 0.8224\n",
      "Epoch 13/15\n",
      "624/624 - 65s - loss: 0.0219 - auc: 0.8825 - lr: 0.0010 - val_loss: 0.0110 - val_auc: 0.7106\n",
      "Epoch 14/15\n",
      "624/624 - 65s - loss: 0.0292 - auc: 0.8358 - lr: 0.0010 - val_loss: 0.0090 - val_auc: 0.8196\n",
      "Epoch 15/15\n",
      "624/624 - 65s - loss: 0.0294 - auc: 0.8440 - lr: 0.0010 - val_loss: 0.0081 - val_auc: 0.8152\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 5 completed. PR-AUC score : 0.86375\n",
      "Training completed. oof PR-AUC score : 0.87200\n",
      "PR-AUC socre average : 0.89628, std : 0.05932\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "print(f'Training WaveNet model with {SPLITS} folds Started...')\n",
    "run_cv_model_by_batch(train, test, SPLITS, features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "print('Training completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPRGufJ-8vxd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wavenet_ex10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
