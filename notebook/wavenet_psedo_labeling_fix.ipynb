{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 977,
     "status": "ok",
     "timestamp": 1591719376849,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "-pgnOVI7LepI",
    "outputId": "0024ac0f-8962-4f4f-8d84-97034c01d3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  9 16:16:16 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taki8e9ZS3_W"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, models, optimizers\n",
    "from tensorflow.keras.callbacks import Callback, LearningRateScheduler\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fpcgGmsLsbg"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TtoHj-hvTIxD"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "TF-Keras SWA: callback utility for performing stochastic weight averaging (SWA).\n",
    "from https://github.com/simon-larsson/keras-swa\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "class SWA(Callback):\n",
    "    \"\"\" Stochastic Weight Averging.\n",
    "    # Paper\n",
    "        title: Averaging Weights Leads to Wider Optima and Better Generalization\n",
    "        link: https://arxiv.org/abs/1803.05407\n",
    "    # Arguments\n",
    "        start_epoch:   integer, epoch when swa should start.\n",
    "        lr_schedule:   string, type of learning rate schedule.\n",
    "        swa_lr:        float, learning rate for swa sampling.\n",
    "        swa_lr2:       float, upper bound of cyclic learning rate.\n",
    "        swa_freq:      integer, length of learning rate cycle.\n",
    "        verbose:       integer, verbosity mode, 0 or 1.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 start_epoch,\n",
    "                 lr_schedule='manual',\n",
    "                 swa_lr='auto',\n",
    "                 swa_lr2='auto',\n",
    "                 swa_freq=1,\n",
    "                 verbose=0):\n",
    "                 \n",
    "        super(SWA, self).__init__()\n",
    "        self.start_epoch = start_epoch - 1\n",
    "        self.lr_schedule = lr_schedule\n",
    "        self.swa_lr = swa_lr\n",
    "        self.swa_lr2 = swa_lr2\n",
    "        self.swa_freq = swa_freq\n",
    "        self.verbose = verbose\n",
    "\n",
    "        if start_epoch < 2:\n",
    "            raise ValueError('\"swa_start\" attribute cannot be lower than 2.')\n",
    "\n",
    "        schedules = ['manual', 'constant', 'cyclic']\n",
    "\n",
    "        if self.lr_schedule not in schedules:\n",
    "            raise ValueError('\"{}\" is not a valid learning rate schedule' \\\n",
    "                             .format(self.lr_schedule))\n",
    "\n",
    "        if self.lr_schedule == 'cyclic' and self.swa_freq < 2:\n",
    "            raise ValueError('\"swa_freq\" must be higher than 1 for cyclic schedule.')\n",
    "\n",
    "        if self.swa_lr == 'auto' and self.swa_lr2 != 'auto':\n",
    "            raise ValueError('\"swa_lr2\" cannot be manually set if \"swa_lr\" is automatic.') \n",
    "            \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr != 'auto' \\\n",
    "           and self.swa_lr2 != 'auto' and self.swa_lr > self.swa_lr2:\n",
    "            raise ValueError('\"swa_lr\" must be lower than \"swa_lr2\".')\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.epochs = self.params.get('epochs')\n",
    "\n",
    "        if self.start_epoch >= self.epochs - 1:\n",
    "            raise ValueError('\"swa_start\" attribute must be lower than \"epochs\".')\n",
    "\n",
    "        self.init_lr = K.eval(self.model.optimizer.lr)\n",
    "\n",
    "        # automatic swa_lr\n",
    "        if self.swa_lr == 'auto':\n",
    "            self.swa_lr = 0.1*self.init_lr\n",
    "        \n",
    "        if self.init_lr < self.swa_lr:\n",
    "            raise ValueError('\"swa_lr\" must be lower than rate set in optimizer.')\n",
    "\n",
    "        # automatic swa_lr2 between initial lr and swa_lr   \n",
    "        if self.lr_schedule == 'cyclic' and self.swa_lr2 == 'auto':\n",
    "            self.swa_lr2 = self.swa_lr + (self.init_lr - self.swa_lr)*0.25\n",
    "\n",
    "        self._check_batch_norm()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "\n",
    "        self.current_epoch = epoch\n",
    "        self._scheduler(epoch)\n",
    "\n",
    "        # constant schedule is updated epoch-wise\n",
    "        if self.lr_schedule == 'constant' or self.is_batch_norm_epoch:\n",
    "            self._update_lr(epoch)\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_weights = self.model.get_weights()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: starting stochastic weight averaging'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            self._set_swa_weights(epoch)\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: reinitializing batch normalization layers'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "            self._reset_batch_norm()\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print('\\nEpoch %05d: running forward pass to adjust batch normalization'\n",
    "                      % (epoch + 1))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "\n",
    "        # update lr each batch for cyclic lr schedule\n",
    "        if self.lr_schedule == 'cyclic':\n",
    "            self._update_lr(self.current_epoch, batch)\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "\n",
    "            batch_size = self.params['samples']\n",
    "            momentum = batch_size / (batch*batch_size + batch_size)\n",
    "\n",
    "            for layer in self.batch_norm_layers:\n",
    "                layer.momentum = momentum\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.eval(self.model.optimizer.lr)\n",
    "        for k, v in logs.items():\n",
    "            if k == 'lr':\n",
    "                self.model.history.history.setdefault(k, []).append(v)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        if self.is_swa_start_epoch:\n",
    "            self.swa_start_epoch = epoch\n",
    "\n",
    "        if self.is_swa_epoch and not self.is_batch_norm_epoch:\n",
    "            self.swa_weights = self._average_weights(epoch)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "\n",
    "        if not self.has_batch_norm:\n",
    "            self._set_swa_weights(self.epochs)\n",
    "        else:\n",
    "            self._restore_batch_norm()\n",
    "\n",
    "    def _scheduler(self, epoch):\n",
    "\n",
    "        swa_epoch = (epoch - self.start_epoch)\n",
    "\n",
    "        self.is_swa_epoch = epoch >= self.start_epoch and swa_epoch % self.swa_freq == 0\n",
    "        self.is_swa_start_epoch = epoch == self.start_epoch\n",
    "        self.is_batch_norm_epoch = epoch == self.epochs - 1 and self.has_batch_norm\n",
    "\n",
    "    def _average_weights(self, epoch):\n",
    "\n",
    "        return [(swa_w * (epoch - self.start_epoch) + w)\n",
    "                / ((epoch - self.start_epoch) + 1)\n",
    "                for swa_w, w in zip(self.swa_weights, self.model.get_weights())]\n",
    "\n",
    "    def _update_lr(self, epoch, batch=None):\n",
    "\n",
    "        if self.is_batch_norm_epoch:\n",
    "            lr = 0\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'constant':\n",
    "            lr = self._constant_schedule(epoch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "        elif self.lr_schedule == 'cyclic':\n",
    "            lr = self._cyclic_schedule(epoch, batch)\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "    def _constant_schedule(self, epoch):\n",
    "\n",
    "        t = epoch / self.start_epoch\n",
    "        lr_ratio = self.swa_lr / self.init_lr\n",
    "        if t <= 0.5:\n",
    "            factor = 1.0\n",
    "        elif t <= 0.9:\n",
    "            factor = 1.0 - (1.0 - lr_ratio) * (t - 0.5) / 0.4\n",
    "        else:\n",
    "            factor = lr_ratio\n",
    "        return self.init_lr * factor\n",
    "\n",
    "    def _cyclic_schedule(self, epoch, batch):\n",
    "        \"\"\" Designed after Section 3.1 of Averaging Weights Leads to\n",
    "        Wider Optima and Better Generalization(https://arxiv.org/abs/1803.05407)\n",
    "        \"\"\"\n",
    "        # steps are mini-batches per epoch, equal to training_samples / batch_size\n",
    "        steps = self.params.get('steps')\n",
    "        \n",
    "        #occasionally steps parameter will not be set. We then calculate it ourselves\n",
    "        if steps == None:\n",
    "            steps = self.params['samples'] // self.params['batch_size']\n",
    "        \n",
    "        swa_epoch = (epoch - self.start_epoch) % self.swa_freq\n",
    "        cycle_length = self.swa_freq * steps\n",
    "\n",
    "        # batch 0 indexed, so need to add 1\n",
    "        i = (swa_epoch * steps) + (batch + 1)\n",
    "        if epoch >= self.start_epoch:\n",
    "            t = (((i-1) % cycle_length) + 1)/cycle_length\n",
    "            return (1-t)*self.swa_lr2 + t*self.swa_lr\n",
    "        else:\n",
    "            return self._constant_schedule(epoch)\n",
    "\n",
    "    def _set_swa_weights(self, epoch):\n",
    "\n",
    "        self.model.set_weights(self.swa_weights)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: final model weights set to stochastic weight average'\n",
    "                  % (epoch + 1))\n",
    "\n",
    "    def _check_batch_norm(self):\n",
    "\n",
    "        self.batch_norm_momentums = []\n",
    "        self.batch_norm_layers = []\n",
    "        self.has_batch_norm = False\n",
    "        self.running_bn_epoch = False\n",
    "\n",
    "        for layer in self.model.layers:\n",
    "            if issubclass(layer.__class__, BatchNormalization):\n",
    "                self.has_batch_norm = True\n",
    "                self.batch_norm_momentums.append(layer.momentum)\n",
    "                self.batch_norm_layers.append(layer)\n",
    "\n",
    "        if self.verbose > 0 and self.has_batch_norm:\n",
    "            print('Model uses batch normalization. SWA will require last epoch '\n",
    "                  'to be a forward pass and will run with no learning rate')\n",
    "\n",
    "    def _reset_batch_norm(self):\n",
    "\n",
    "        for layer in self.batch_norm_layers:\n",
    "\n",
    "            # to get properly initialized moving mean and moving variance weights\n",
    "            # we initialize a new batch norm layer from the config of the existing\n",
    "            # layer, build that layer, retrieve its reinitialized moving mean and\n",
    "            # moving var weights and then delete the layer\n",
    "            bn_config = layer.get_config()\n",
    "            new_batch_norm = BatchNormalization(**bn_config)\n",
    "            new_batch_norm.build(layer.input_shape)\n",
    "            new_moving_mean, new_moving_var = new_batch_norm.get_weights()[-2:]\n",
    "            # get rid of the new_batch_norm layer\n",
    "            del new_batch_norm\n",
    "            # get the trained gamma and beta from the current batch norm layer\n",
    "            trained_weights = layer.get_weights()\n",
    "            new_weights = []\n",
    "            # get gamma if exists\n",
    "            if bn_config['scale']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            # get beta if exists\n",
    "            if bn_config['center']:\n",
    "                new_weights.append(trained_weights.pop(0))\n",
    "            new_weights += [new_moving_mean, new_moving_var]\n",
    "            # set weights to trained gamma and beta, reinitialized mean and variance\n",
    "            layer.set_weights(new_weights)\n",
    "\n",
    "    def _restore_batch_norm(self):\n",
    "\n",
    "        for layer, momentum in zip(self.batch_norm_layers, self.batch_norm_momentums):\n",
    "            layer.momentum = momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_U98VvSmVp6"
   },
   "outputs": [],
   "source": [
    "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
    "    labels_num = np.max(y) + 1\n",
    "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
    "    y_distr = Counter()\n",
    "    for label, g in zip(y, groups):\n",
    "        y_counts_per_group[g][label] += 1\n",
    "        y_distr[label] += 1\n",
    "\n",
    "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
    "    groups_per_fold = defaultdict(set)\n",
    "\n",
    "    def eval_y_counts_per_fold(y_counts, fold):\n",
    "        y_counts_per_fold[fold] += y_counts\n",
    "        std_per_label = []\n",
    "        for label in range(labels_num):\n",
    "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
    "            std_per_label.append(label_std)\n",
    "        y_counts_per_fold[fold] -= y_counts\n",
    "        return np.mean(std_per_label)\n",
    "\n",
    "    groups_and_y_counts = list(y_counts_per_group.items())\n",
    "    random.Random(seed).shuffle(groups_and_y_counts)\n",
    "\n",
    "    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n",
    "        best_fold = None\n",
    "        min_eval = None\n",
    "        for i in range(k):\n",
    "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
    "            if min_eval is None or fold_eval < min_eval:\n",
    "                min_eval = fold_eval\n",
    "                best_fold = i\n",
    "        y_counts_per_fold[best_fold] += y_counts\n",
    "        groups_per_fold[best_fold].add(g)\n",
    "\n",
    "    all_groups = set(groups)\n",
    "    for i in range(k):\n",
    "        train_groups = all_groups - groups_per_fold[i]\n",
    "        test_groups = groups_per_fold[i]\n",
    "\n",
    "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
    "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
    "\n",
    "        yield train_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWsEUOXvTYry"
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('../input/train_spec.csv')\n",
    "    test  = pd.read_csv('../input/test_spec.csv')\n",
    "    sub  = pd.read_csv('../input/atmaCup5__sample_submission.csv')\n",
    "    return train, test, sub\n",
    "\n",
    "def Scaling(train,test,features):\n",
    "    train_input_mean = train[features].mean(axis=1)\n",
    "    train_input_sigma = train[features].std(axis=1)\n",
    "    test_input_mean = test[features].mean(axis=1)\n",
    "    test_input_sigma = test[features].std(axis=1)\n",
    "    train[features]= train[features].sub(train_input_mean, axis=0).div(train_input_sigma, axis=0)\n",
    "    test[features] = test[features].sub(test_input_mean, axis=0).div(test_input_sigma, axis=0)\n",
    "    return train, test\n",
    "\n",
    "def Scaling_single(df, features):\n",
    "    df_input_mean = df[features].mean(axis=1)\n",
    "    df_input_sigma = df[features].std(axis=1)\n",
    "    df[features]= df[features].sub(df_input_mean, axis=0).div(df_input_sigma, axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ2z1yf3abO1"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    return LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1l7joOCeuiQX"
   },
   "outputs": [],
   "source": [
    "def Classifier():\n",
    "    def wave_block(x, filters, kernel_size, n):\n",
    "        dilation_rates = [2**i for i in range(n)]\n",
    "        x = Conv1D(filters = filters,\n",
    "                   kernel_size = 1,\n",
    "                   padding = 'same')(x)\n",
    "        res_x = x\n",
    "        for dilation_rate in dilation_rates:\n",
    "            tanh_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same', \n",
    "                              activation = 'tanh', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            sigm_out = Conv1D(filters = filters,\n",
    "                              kernel_size = kernel_size,\n",
    "                              padding = 'same',\n",
    "                              activation = 'sigmoid', \n",
    "                              dilation_rate = dilation_rate)(x)\n",
    "            x = Multiply()([tanh_out, sigm_out])\n",
    "            x = Conv1D(filters = filters,\n",
    "                       kernel_size = 1,\n",
    "                       padding = 'same')(x)\n",
    "            res_x = Add()([res_x, x])\n",
    "        return res_x\n",
    "    \n",
    "    inp = Input(shape = (512, 1))\n",
    "\n",
    "    x = wave_block(inp, 16, 5, 8)\n",
    "    x = wave_block(x, 32, 5, 12)\n",
    "    x = wave_block(x, 64, 5, 4)\n",
    "    x = wave_block(x, 128, 5, 1)\n",
    "\n",
    "    x_max = GlobalMaxPooling1D()(x)\n",
    "    x_mean = GlobalAveragePooling1D()(x)\n",
    "    x = Concatenate()([x_max, x_mean])\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(32, activation = 'relu')(x)\n",
    "    out = Dense(1, activation = 'sigmoid', name = 'out')(x)\n",
    "    \n",
    "    model = models.Model(inputs = inp, outputs = out)\n",
    "    \n",
    "    opt = Adam(lr=LR)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[pr_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdURnaTqvPdj"
   },
   "outputs": [],
   "source": [
    "def run_cv_model_by_batch(train, test, splits, feats, sample_submission, nn_epochs, nn_batch_size):\n",
    "    seed_everything(SEED)\n",
    "    K.clear_session()\n",
    "    config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1,inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=config)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    oof = np.load(\"../output/oof.0.86263.npy\")\n",
    "    sub = np.load(\"../output/preds.0.86263.npy\")    \n",
    "    pused_test = test.copy()\n",
    "    pused_test['target'] = sub\n",
    "    pused_test = pused_test[(sub >= 0.99) | (sub <= 0.01)].copy().reset_index(drop=True)\n",
    "    pused_test.loc[ pused_test['target']>=0.5, 'target' ] = 1\n",
    "    pused_test.loc[ pused_test['target']<0.5, 'target' ] = 0\n",
    "\n",
    "    pused_test = Scaling_single(pused_test, features)\n",
    "\n",
    "    pused_te_tr = pused_test[target].values\n",
    "    pused_test_x = pused_test[features].values\n",
    "      \n",
    "    oof_ = np.zeros((len(train), len(target)))\n",
    "    preds_ = np.zeros((len(test), len(target)))\n",
    "    train['chip_id'], _ = pd.factorize(train['chip_id'])\n",
    "    splits = [x for x in stratified_group_k_fold(train, train.target.astype(int), train.chip_id, k=SPLITS, seed=SEED)]\n",
    "    score_list = []\n",
    "\n",
    "    train_tr = train[target]\n",
    "    train = train[features]\n",
    "    test = test[features]\n",
    "    \n",
    "    for n_fold, (tr_idx, val_idx) in enumerate(splits):\n",
    "        train_x, train_y = train.iloc[tr_idx].values, train_tr.iloc[tr_idx].values\n",
    "        valid_x, valid_y = train.iloc[val_idx].values, train_tr.iloc[val_idx].values\n",
    "\n",
    "        train_x = np.concatenate([train_x, pused_test_x], axis=0)\n",
    "        train_y = np.concatenate([train_y, pused_te_tr], axis=0)\n",
    "\n",
    "        train_x = train_x.reshape(train_x.shape[0], train_x.shape[1], -1)\n",
    "        valid_x = valid_x.reshape(valid_x.shape[0], valid_x.shape[1], -1)\n",
    "        train_y = train_y.reshape(-1)\n",
    "        valid_y = valid_y.reshape(-1)\n",
    "        print(f'Our training dataset shape is {train_x.shape}')\n",
    "        print(f'Our validation dataset shape is {valid_x.shape}')\n",
    "\n",
    "        model = Classifier()\n",
    "\n",
    "        cb_lr_schedule = LearningRateScheduler(lr_schedule)\n",
    "        model.fit(train_x, train_y,\n",
    "                  epochs = nn_epochs,\n",
    "                  callbacks = [swa, cb_lr_schedule],\n",
    "                  batch_size = nn_batch_size, verbose = 2,\n",
    "                  validation_data = (valid_x,valid_y))\n",
    "        #model.save_weights(f'./drive/My Drive/atmaCup05/output/ex10-WaveNet-{n_fold}-fix.h5')\n",
    "        gc.collect()\n",
    "        preds_f = model.predict(valid_x)\n",
    "        score_ = average_precision_score(valid_y,  preds_f)\n",
    "        print(f'Training fold {n_fold + 1} completed. PR-AUC score : {score_ :1.5f}')\n",
    "        preds_f = preds_f.reshape(-1, preds_f.shape[-1])\n",
    "        oof_[val_idx,:] += preds_f\n",
    "        te_preds = model.predict(test)\n",
    "        te_preds = te_preds.reshape(-1, te_preds.shape[-1])           \n",
    "        preds_ += te_preds / SPLITS\n",
    "\n",
    "        score_list.append(score_)\n",
    "    \n",
    "\n",
    "    score_ = average_precision_score(train_tr,  oof_)\n",
    "    print(f'Training completed. oof PR-AUC score : {score_:1.5f}')\n",
    "    print(f'PR-AUC socre average : {np.mean(score_list):1.5f}, std : {np.std(score_list):1.5f}')\n",
    "    \n",
    "    np.save(f'../output/oof.{score_:1.5f}_pseudo_labeling_fix.npy',oof_)\n",
    "    np.save(f'../output/preds.{score_:1.5f}_pseudo_labeling_fix.npy',preds_)\n",
    "    \n",
    "    \n",
    "    sample_submission['target'] = preds_\n",
    "    sample_submission.to_csv(f'../output/submission_wavenet.{score_:1.5f}_pseudo_labeling_fix.csv', index=False)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    lr_precision, lr_recall, _ = metrics.precision_recall_curve(train_tr,  oof_)\n",
    "    ax.plot(lr_recall, lr_precision, marker='.', label='my prediction')\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMiD5CeTTTR7"
   },
   "outputs": [],
   "source": [
    "## config\n",
    "EPOCHS = 15\n",
    "start_epoch = 10\n",
    "NNBATCHSIZE = 16\n",
    "SEED = 1234\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "target = ['target']\n",
    "\n",
    "swa = SWA(start_epoch=start_epoch, \n",
    "          lr_schedule='manual',\n",
    "          swa_lr=0.001, \n",
    "          verbose=1)\n",
    "\n",
    "pr_metric = AUC(curve='PR', num_thresholds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5093,
     "status": "ok",
     "timestamp": 1591719381068,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "U_uljJeMvedC",
    "outputId": "a283aa53-cfeb-4116-88d4-4f773d5cfdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reading Data Started...')\n",
    "train, test, sample_submission = read_data()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZ4Zpyfu5pUT"
   },
   "outputs": [],
   "source": [
    "features=[col for col in test.columns if \"wave\" in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfQRmbQ9v_DT"
   },
   "outputs": [],
   "source": [
    "train, test = Scaling(train, test, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3450470,
     "status": "ok",
     "timestamp": 1591722826525,
     "user": {
      "displayName": "米川和仁",
      "photoUrl": "",
      "userId": "04802509165152586358"
     },
     "user_tz": -540
    },
    "id": "Wrrccx2XwblD",
    "outputId": "d5620b30-ef5c-4e71-ff84-d787611377c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Conv1d model with 5 folds Started...\n",
      "Our training dataset shape is (11920, 512, 1)\n",
      "Our validation dataset shape is (1805, 512, 1)\n",
      "Epoch 1/15\n",
      "745/745 - 44s - loss: 0.0612 - auc: 0.3063 - lr: 0.0010 - val_loss: 0.0463 - val_auc: 0.8810\n",
      "Epoch 2/15\n",
      "745/745 - 43s - loss: 0.0349 - auc: 0.6191 - lr: 0.0010 - val_loss: 0.0403 - val_auc: 0.8998\n",
      "Epoch 3/15\n",
      "745/745 - 43s - loss: 0.0254 - auc: 0.7465 - lr: 0.0010 - val_loss: 0.0424 - val_auc: 0.9025\n",
      "Epoch 4/15\n",
      "745/745 - 43s - loss: 0.0265 - auc: 0.7249 - lr: 0.0010 - val_loss: 0.0664 - val_auc: 0.9040\n",
      "Epoch 5/15\n",
      "745/745 - 43s - loss: 0.0199 - auc: 0.8100 - lr: 0.0010 - val_loss: 0.0349 - val_auc: 0.9219\n",
      "Epoch 6/15\n",
      "745/745 - 43s - loss: 0.0220 - auc: 0.7624 - lr: 0.0010 - val_loss: 0.0327 - val_auc: 0.9184\n",
      "Epoch 7/15\n",
      "745/745 - 43s - loss: 0.0192 - auc: 0.8370 - lr: 0.0010 - val_loss: 0.0320 - val_auc: 0.9162\n",
      "Epoch 8/15\n",
      "745/745 - 43s - loss: 0.0170 - auc: 0.8472 - lr: 0.0010 - val_loss: 0.0333 - val_auc: 0.9166\n",
      "Epoch 9/15\n",
      "745/745 - 43s - loss: 0.0180 - auc: 0.8221 - lr: 0.0010 - val_loss: 0.0311 - val_auc: 0.9243\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "745/745 - 43s - loss: 0.0188 - auc: 0.8298 - lr: 0.0010 - val_loss: 0.0483 - val_auc: 0.9324\n",
      "Epoch 11/15\n",
      "745/745 - 43s - loss: 0.0180 - auc: 0.8274 - lr: 0.0010 - val_loss: 0.0330 - val_auc: 0.9230\n",
      "Epoch 12/15\n",
      "745/745 - 43s - loss: 0.0192 - auc: 0.8265 - lr: 0.0010 - val_loss: 0.0360 - val_auc: 0.9164\n",
      "Epoch 13/15\n",
      "745/745 - 43s - loss: 0.0175 - auc: 0.8414 - lr: 0.0010 - val_loss: 0.0414 - val_auc: 0.9014\n",
      "Epoch 14/15\n",
      "745/745 - 43s - loss: 0.0160 - auc: 0.8530 - lr: 0.0010 - val_loss: 0.0446 - val_auc: 0.9243\n",
      "Epoch 15/15\n",
      "745/745 - 43s - loss: 0.0164 - auc: 0.8650 - lr: 0.0010 - val_loss: 0.0345 - val_auc: 0.9243\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 1 completed. PR-AUC score : 0.92636\n",
      "Our training dataset shape is (11904, 512, 1)\n",
      "Our validation dataset shape is (1821, 512, 1)\n",
      "Epoch 1/15\n",
      "744/744 - 44s - loss: 0.0604 - auc: 0.4549 - lr: 0.0010 - val_loss: 0.1304 - val_auc: 0.7818\n",
      "Epoch 2/15\n",
      "744/744 - 43s - loss: 0.0268 - auc: 0.6160 - lr: 0.0010 - val_loss: 0.2137 - val_auc: 0.7764\n",
      "Epoch 3/15\n",
      "744/744 - 43s - loss: 0.0203 - auc: 0.7423 - lr: 0.0010 - val_loss: 0.0856 - val_auc: 0.8431\n",
      "Epoch 4/15\n",
      "744/744 - 43s - loss: 0.0221 - auc: 0.7301 - lr: 0.0010 - val_loss: 0.0862 - val_auc: 0.8459\n",
      "Epoch 5/15\n",
      "744/744 - 43s - loss: 0.0206 - auc: 0.7056 - lr: 0.0010 - val_loss: 0.0737 - val_auc: 0.8652\n",
      "Epoch 6/15\n",
      "744/744 - 43s - loss: 0.0188 - auc: 0.7816 - lr: 0.0010 - val_loss: 0.1425 - val_auc: 0.8470\n",
      "Epoch 7/15\n",
      "744/744 - 43s - loss: 0.0160 - auc: 0.7888 - lr: 0.0010 - val_loss: 0.0828 - val_auc: 0.8682\n",
      "Epoch 8/15\n",
      "744/744 - 43s - loss: 0.0157 - auc: 0.8146 - lr: 0.0010 - val_loss: 0.1000 - val_auc: 0.8429\n",
      "Epoch 9/15\n",
      "744/744 - 43s - loss: 0.0200 - auc: 0.7667 - lr: 0.0010 - val_loss: 0.0948 - val_auc: 0.8583\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "744/744 - 43s - loss: 0.0141 - auc: 0.8142 - lr: 0.0010 - val_loss: 0.0921 - val_auc: 0.8262\n",
      "Epoch 11/15\n",
      "744/744 - 43s - loss: 0.0168 - auc: 0.7860 - lr: 0.0010 - val_loss: 0.1298 - val_auc: 0.8429\n",
      "Epoch 12/15\n",
      "744/744 - 43s - loss: 0.0158 - auc: 0.8130 - lr: 0.0010 - val_loss: 0.0858 - val_auc: 0.8655\n",
      "Epoch 13/15\n",
      "744/744 - 43s - loss: 0.0133 - auc: 0.8470 - lr: 0.0010 - val_loss: 0.0944 - val_auc: 0.8689\n",
      "Epoch 14/15\n",
      "744/744 - 43s - loss: 0.0119 - auc: 0.8593 - lr: 0.0010 - val_loss: 0.1579 - val_auc: 0.8238\n",
      "Epoch 15/15\n",
      "744/744 - 43s - loss: 0.0136 - auc: 0.8391 - lr: 0.0010 - val_loss: 0.0950 - val_auc: 0.8385\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 2 completed. PR-AUC score : 0.86993\n",
      "Our training dataset shape is (12531, 512, 1)\n",
      "Our validation dataset shape is (1194, 512, 1)\n",
      "Epoch 1/15\n",
      "784/784 - 48s - loss: 0.0664 - auc: 0.5450 - lr: 0.0010 - val_loss: 0.0108 - val_auc: 0.9357\n",
      "Epoch 2/15\n",
      "784/784 - 46s - loss: 0.0397 - auc: 0.6836 - lr: 0.0010 - val_loss: 0.0187 - val_auc: 0.8776\n",
      "Epoch 3/15\n",
      "784/784 - 47s - loss: 0.0308 - auc: 0.7672 - lr: 0.0010 - val_loss: 0.0154 - val_auc: 0.9350\n",
      "Epoch 4/15\n",
      "784/784 - 46s - loss: 0.0281 - auc: 0.7974 - lr: 0.0010 - val_loss: 0.0324 - val_auc: 0.9156\n",
      "Epoch 5/15\n",
      "784/784 - 47s - loss: 0.0247 - auc: 0.8228 - lr: 0.0010 - val_loss: 0.0110 - val_auc: 0.9316\n",
      "Epoch 6/15\n",
      "784/784 - 46s - loss: 0.0250 - auc: 0.8195 - lr: 0.0010 - val_loss: 0.0107 - val_auc: 0.9317\n",
      "Epoch 7/15\n",
      "784/784 - 47s - loss: 0.0241 - auc: 0.8312 - lr: 0.0010 - val_loss: 0.0120 - val_auc: 0.8892\n",
      "Epoch 8/15\n",
      "784/784 - 47s - loss: 0.0226 - auc: 0.8556 - lr: 0.0010 - val_loss: 0.0127 - val_auc: 0.9611\n",
      "Epoch 9/15\n",
      "784/784 - 46s - loss: 0.0229 - auc: 0.8405 - lr: 0.0010 - val_loss: 0.0125 - val_auc: 0.8758\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "784/784 - 46s - loss: 0.0225 - auc: 0.8486 - lr: 0.0010 - val_loss: 0.0170 - val_auc: 0.9125\n",
      "Epoch 11/15\n",
      "784/784 - 46s - loss: 0.0212 - auc: 0.8572 - lr: 0.0010 - val_loss: 0.0072 - val_auc: 0.9521\n",
      "Epoch 12/15\n",
      "784/784 - 47s - loss: 0.0201 - auc: 0.8736 - lr: 0.0010 - val_loss: 0.0124 - val_auc: 0.9596\n",
      "Epoch 13/15\n",
      "784/784 - 46s - loss: 0.0214 - auc: 0.8635 - lr: 0.0010 - val_loss: 0.0144 - val_auc: 0.9161\n",
      "Epoch 14/15\n",
      "784/784 - 46s - loss: 0.0189 - auc: 0.8663 - lr: 0.0010 - val_loss: 0.0151 - val_auc: 0.9212\n",
      "Epoch 15/15\n",
      "784/784 - 46s - loss: 0.0193 - auc: 0.8650 - lr: 0.0010 - val_loss: 0.0072 - val_auc: 0.9841\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 3 completed. PR-AUC score : 0.98464\n",
      "Our training dataset shape is (12577, 512, 1)\n",
      "Our validation dataset shape is (1148, 512, 1)\n",
      "Epoch 1/15\n",
      "787/787 - 48s - loss: 0.0618 - auc: 0.5303 - lr: 0.0010 - val_loss: 0.0558 - val_auc: 0.5602\n",
      "Epoch 2/15\n",
      "787/787 - 46s - loss: 0.0318 - auc: 0.7829 - lr: 0.0010 - val_loss: 0.0218 - val_auc: 0.5306\n",
      "Epoch 3/15\n",
      "787/787 - 46s - loss: 0.0278 - auc: 0.8007 - lr: 0.0010 - val_loss: 0.0323 - val_auc: 0.5059\n",
      "Epoch 4/15\n",
      "787/787 - 46s - loss: 0.0277 - auc: 0.7934 - lr: 0.0010 - val_loss: 0.0252 - val_auc: 0.6371\n",
      "Epoch 5/15\n",
      "787/787 - 46s - loss: 0.0235 - auc: 0.8317 - lr: 0.0010 - val_loss: 0.0294 - val_auc: 0.6417\n",
      "Epoch 6/15\n",
      "787/787 - 46s - loss: 0.0239 - auc: 0.8127 - lr: 0.0010 - val_loss: 0.0230 - val_auc: 0.5180\n",
      "Epoch 7/15\n",
      "787/787 - 46s - loss: 0.0229 - auc: 0.8501 - lr: 0.0010 - val_loss: 0.0212 - val_auc: 0.7909\n",
      "Epoch 8/15\n",
      "787/787 - 46s - loss: 0.0224 - auc: 0.8388 - lr: 0.0010 - val_loss: 0.0232 - val_auc: 0.7333\n",
      "Epoch 9/15\n",
      "787/787 - 46s - loss: 0.0217 - auc: 0.8534 - lr: 0.0010 - val_loss: 0.0265 - val_auc: 0.7529\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "787/787 - 46s - loss: 0.0194 - auc: 0.8696 - lr: 0.0010 - val_loss: 0.0217 - val_auc: 0.8054\n",
      "Epoch 11/15\n",
      "787/787 - 46s - loss: 0.0208 - auc: 0.8567 - lr: 0.0010 - val_loss: 0.0219 - val_auc: 0.7324\n",
      "Epoch 12/15\n",
      "787/787 - 46s - loss: 0.0201 - auc: 0.8687 - lr: 0.0010 - val_loss: 0.0287 - val_auc: 0.5430\n",
      "Epoch 13/15\n",
      "787/787 - 46s - loss: 0.0196 - auc: 0.8806 - lr: 0.0010 - val_loss: 0.0309 - val_auc: 0.6621\n",
      "Epoch 14/15\n",
      "787/787 - 46s - loss: 0.0192 - auc: 0.8895 - lr: 0.0010 - val_loss: 0.0256 - val_auc: 0.6663\n",
      "Epoch 15/15\n",
      "787/787 - 46s - loss: 0.0174 - auc: 0.8869 - lr: 0.0010 - val_loss: 0.0242 - val_auc: 0.6653\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 4 completed. PR-AUC score : 0.71412\n",
      "Our training dataset shape is (12257, 512, 1)\n",
      "Our validation dataset shape is (1468, 512, 1)\n",
      "Epoch 1/15\n",
      "767/767 - 47s - loss: 0.0585 - auc: 0.4944 - lr: 0.0010 - val_loss: 0.0440 - val_auc: 0.6413\n",
      "Epoch 2/15\n",
      "767/767 - 45s - loss: 0.0356 - auc: 0.7316 - lr: 0.0010 - val_loss: 0.0235 - val_auc: 0.8703\n",
      "Epoch 3/15\n",
      "767/767 - 45s - loss: 0.0263 - auc: 0.8108 - lr: 0.0010 - val_loss: 0.0492 - val_auc: 0.7920\n",
      "Epoch 4/15\n",
      "767/767 - 45s - loss: 0.0265 - auc: 0.8008 - lr: 0.0010 - val_loss: 0.0441 - val_auc: 0.7239\n",
      "Epoch 5/15\n",
      "767/767 - 45s - loss: 0.0245 - auc: 0.8244 - lr: 0.0010 - val_loss: 0.0265 - val_auc: 0.7931\n",
      "Epoch 6/15\n",
      "767/767 - 45s - loss: 0.0256 - auc: 0.8158 - lr: 0.0010 - val_loss: 0.0325 - val_auc: 0.7833\n",
      "Epoch 7/15\n",
      "767/767 - 45s - loss: 0.0216 - auc: 0.8489 - lr: 0.0010 - val_loss: 0.0286 - val_auc: 0.7521\n",
      "Epoch 8/15\n",
      "767/767 - 45s - loss: 0.0202 - auc: 0.8711 - lr: 0.0010 - val_loss: 0.0272 - val_auc: 0.7224\n",
      "Epoch 9/15\n",
      "767/767 - 45s - loss: 0.0191 - auc: 0.8677 - lr: 0.0010 - val_loss: 0.0295 - val_auc: 0.7061\n",
      "\n",
      "Epoch 00010: starting stochastic weight averaging\n",
      "Epoch 10/15\n",
      "767/767 - 45s - loss: 0.0193 - auc: 0.8743 - lr: 0.0010 - val_loss: 0.0231 - val_auc: 0.7946\n",
      "Epoch 11/15\n",
      "767/767 - 45s - loss: 0.0211 - auc: 0.8560 - lr: 0.0010 - val_loss: 0.0236 - val_auc: 0.8048\n",
      "Epoch 12/15\n",
      "767/767 - 45s - loss: 0.0187 - auc: 0.8789 - lr: 0.0010 - val_loss: 0.0588 - val_auc: 0.8011\n",
      "Epoch 13/15\n",
      "767/767 - 45s - loss: 0.0196 - auc: 0.8772 - lr: 0.0010 - val_loss: 0.0260 - val_auc: 0.7770\n",
      "Epoch 14/15\n",
      "767/767 - 45s - loss: 0.0202 - auc: 0.8623 - lr: 0.0010 - val_loss: 0.0213 - val_auc: 0.7739\n",
      "Epoch 15/15\n",
      "767/767 - 45s - loss: 0.0199 - auc: 0.8694 - lr: 0.0010 - val_loss: 0.0245 - val_auc: 0.7971\n",
      "\n",
      "Epoch 00016: final model weights set to stochastic weight average\n",
      "Training fold 5 completed. PR-AUC score : 0.88306\n",
      "Training completed. oof PR-AUC score : 0.85306\n",
      "PR-AUC socre average : 0.87562, std : 0.09014\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed...\n"
     ]
    }
   ],
   "source": [
    "print(f'Training WaveNet model with {SPLITS} folds Started...')\n",
    "run_cv_model_by_batch(train, test, SPLITS, features, sample_submission, EPOCHS, NNBATCHSIZE)\n",
    "print('Training completed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_KfKZ5Q3QwP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "wavenet_ex10_fix.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
